# A playground for studying simple Transformer models

This repository contains the following:
* An implementation of a stacked Transformer in PyTorch
* Surrounding utilities to make experimentation easy
* A GPU-capable Docker environment to run Jupyterlab

Much of the original code and thoughtspace comes from the [Grokking work by Nanda et
al](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking#Speculation__Phase_Changes_are_Everywhere)

I'm using this codebase to explore fundamental questions of how Transformers learn, such as
in the above paper. If you're randomly curious, I will update (at least occasionally) [this Notion
document](https://garnet-horn-9ed.notion.site/Research-Outline-and-Notes-275d5fe3c2624d6faeae9f2940e0e2ba)
with findings as I come across them.