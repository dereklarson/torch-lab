# A playground for studying simple Transformer models

This repository contains the following:
* An implementation of a stacked Transformer in PyTorch
* Surrounding utilities to make experimentation easy
* A GPU-capable Docker environment to run Jupyterlab

A companion tool is the ["Transformer Visor"](https://github.com/dereklarson/weight_viz), which lets you visualize the training process
for a small transformer model. See a [live demo here](https://tlab.dereklarson.info)

I'm using this codebase to explore fundamental questions of how Transformers learn, such as
in the interpretability work from [Anthropic](https://www.anthropic.com/#papers) and [Redwood Research](https://www.redwoodresearch.org/research).
